## Motivation



Nowadays, it is difficult to find a business that does not collect data about something; 
humans are particular targets of choice for the *Big Data Movement* 
[@web_2016_privacy-international-about-big-data]. Since humans are all individuals, they are 
distinct from each other. While subsets of individuals might share a minor set of 
attributes, the majority is still very unique to an individual, given that the overall variety of 
attributes is complex. That small amount of similarity might seem to be less 
important, due to the nature of inflationary occurrence, but the opposite turns out to be true. 
These similarities allow to determine the individuals who are part of a subset and the ones who 
aren't. Stereotypical patterns are applied to these subsets and thus to all related individuals. 
This enriched information is then used to help predict outcomes of problems or questions 
related to these individuals. In other words, searching for causation where in best the case one 
might find correlations. This is also known as *discrimination*, which

>   [...] refers to unfair or unequal treatment of people based on membership to a category or a
>   minority, without regard to individual merit 
>   [@paper_2008_discrimination-aware-data-mining, p. 1]. 

Discrimination is a serious issue in our society when humans interact, directly or indirectly, but also when humans leverage computers and algorithms to uncover 
formerly unnoticed information in order to inform their decision making. For example, when 
qualifying for a loan, hiring employees, investigating crimes or renting flats. The decision to 
approve or deny is based on computed data about the individuals in question
[@book_2015_ethical-it-innovation, chap. 5.6], which is merely 
discrimination on a much larger scale and with less effort (almost parenthetically). 
The described phenomenon is originally referred to as *Bias in computer systems*
[@paper_1996_bias-in-computer-systems]. What at first seems like machines going rouge on 
humans is, in fact, the *cognitive bias* [@wikipedia_2016_cognitive-bias] of human nature, modeled 
into machine executable language and built to reveal the patterns their creators were looking for.
The *"Inheritance of humanness"* [@web_2016_big-data-is-people, sec. 2], so to say.

In addition to the identity-defining data mentioned before, humans have the habit to create more and 
more data on a daily basis, both pro-actively (e.g by writing a post) and passively (e.g by allowing 
the app to access their current location while submitting the post). As a result, already gigantic 
databases grow ever larger, waiting to be harvested, collected, aggregated, analyzed 
and finally interpreted. The crux here is, the more data being made available 
[@video_2015_big-data-and-deep-learning_discrimination] to *mine* on, the higher the chances to 
isolate datasets (clusters) that differ from each other but are coherent within themselves.
By defining those datasets, instead of distinguishing on an individual level, humans are being 
reduced to these set-defining characteristics in order to fit in these clusters.

In order to lower potential discrimination, either the responsible parts in these machines need to
be erased while simultaneously raising awareness and teaching people about this issue 
of discrimination, or all the personal data needs to be prevented from falling into these data silos 
in the fist place. Although both approaches are valid and should be pursued simultaneously, the 
latter will be addressed in this work.
